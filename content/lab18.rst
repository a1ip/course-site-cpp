Использование MPI 
#################

:date: 2017-03-02 09:00

.. default-role:: code
.. contents:: Содержание


Основные понятия
================

MPI является наиболее распространенной технологией программирования параллельных компьютеров с распределенной памятью. Основным способом взаимодействия параллельных процессов в таких системах является передача сообщений друг другу. Это и отражено в названии технологии – Message Passing Interface.
Под параллельной программой в рамках MPI понимается множество одновременно выполняемых процессов. Процессы могут выполняться на
разных процессорах, но на одном процессоре могут располагаться и несколько процессов (в этом случае их исполнение осуществляется в
режиме разделения времени). В предельном случае для выполнения параллельной программы может использоваться один процессор – как
правило, такой способ применяется для начальной проверки правильности параллельной программы.
Каждый процесс параллельной программы порождается на основе копии одного и того же программного кода. Данный программный код,
представленный в виде исполняемой программы, должен быть доступен в момент запуска параллельной программы на всех используемых
процессорах.
Исходный программный код для исполняемой программы разрабатывается на языках C или Fortran с применением той или иной реализации библиотеки MPI.
Количество процессов и число используемых процессоров определяется в момент запуска параллельной программы средствами среды исполнения MPI программ и в ходе вычислений не может меняться без применения специальных, но редко задействуемых средств динамического порождения процессов и управления ими, появившихся в стандарте MPI версии 2.0. 
Все процессы программы последовательно перенумерованы от 0 до N, где N есть общее количество процессов. Номер процесса именуется рангом процесса. 
Основу MPI составляют операции передачи сообщений. Среди предусмотренных в составе MPI функций различаются парные (pointtopoint) операции между двумя процессами и коллективные (collective) коммуникационные действия для одновременного взаимодействия нескольких процессов.

Для выполнения парных операций могут использоваться разные режимы передачи, среди которых синхронный, блокирующий и др.
Процессы параллельной программы объединяются в группы. Другим важным понятием MPI, описывающим набор процессов, является понятие
коммуникатора. Под коммуникатором в MPI понимается специально создаваемый служебный объект, который объединяет в своем составе
группу процессов и ряд дополнительных параметров (контекст), используемых при выполнении операций передачи данных.
Парные операции передачи данных выполняются только для процессов, принадлежащих одному и тому же коммуникатору.
Коллективные операции применяются одновременно для всех процессов одного коммуникатора. Как результат, указание используемого
коммуникатора является обязательным для операций передачи данных в MPI.

В ходе вычислений могут создаваться новые и удаляться существующие группы процессов и коммуникаторы. Один и тот же процесс может принадлежать разным группам и коммуникаторам. Все имеющиеся в параллельной программе процессы входят в состав конструируемого по умолчанию коммуникатора с идентификатором MPI_COMM_WORLD.

При выполнении операций передачи сообщений для указания передаваемых или получаемых данных в функциях MPI необходимо указывать тип пересылаемых данных. MPI содержит большой набор базовых типов данных, во многом совпадающих с типами данных в языках C и Fortran. Кроме того, в MPI имеются возможности создания новых производных типов данных для более точного и краткого описания содержимого пересылаемых сообщений.

Итак, если сформулировать коротко, MPI – это библиотека функций, обеспечивающая взаимодействие параллельных процессов с помощью
механизма передачи сообщений. Это библиотека, состоящая примерно из 130 функций, в число которых входят:
* Функции инициализации и закрытия MPI-процессов;
* Функции, реализующие парные операции;
* Функции, реализующие коллективные операции;
* Функции для работы с группами процессов и коммуникаторами;
* Функции для работы со структурами данных;
* Функции формирования топологии процессов.

Изучение MPI начнем с рассмотрения базового набора функций, образующих минимально полный набор, достаточный для написания
простейших программ.


Базовые функции
===============

Любая MPI-программа должна начинаться с вызова функции инициализации MPI – функции MPI_Init. В результате выполнения этой
функции создается группа процессов, в которую помещаются все процессы приложения, и создается область связи, описываемая
предопределенным коммуникатором MPI_COMM_WORLD. 
Эта область связи объединяет все процессы-приложения. Процессы в группе упорядочены и пронумерованы от 0 до N-1, где N равно числу
процессов в группе.

Синтаксис функции инициализации MPI_Init на языке C выглядит следующим образом:

.. code-block:: c

	int MPI_Init(int *argc, char ***argv), где
	* argc — указатель на количество параметров командной строки,
	* argv — параметры командной строки, применяемые для инициализации среды выполнения MPI программы.

Параметрами функции являются количество аргументов в командной строке и адрес указателя на массив символов текста самой
командной строки.

Последней вызываемой функцией MPI обязательно должна являться функция: 

.. code-block:: c

	int MPI_Finalize(void). 

Эта функция закрывает все MPI-процессы и ликвидирует все области связи.

Cтруктура параллельной программы, разработанная с использованием MPI, должна иметь следующий вид:

.. code-block:: c

	#include "mpi.h" // содержит определения именованных констант, прототипов функций и типов данных библиотеки MPI;
	int main(int argc, char *argv[]) {
		<программный код без использования функций MPI>
		MPI_Init(&argc, &argv);
		<программный код с использованием функций MPI>
		MPI_Finalize();
		<программный код без использования функций MPI>
		return 0;
	}

* функции MPI_Init и MPI_Finalize являются обязательными и должны быть выполнены (и только один раз) каждым процессом параллельной программы;
* перед вызовом MPI_Init может быть использована функция MPI_Initialized для определения того, был ли ранее выполнен вызов MPI_Init; 
* после вызова MPI_Finalize – MPI_Finalized аналогичного предназначения.

Рассмотренные примеры функций дают представление синтаксиса именования функций в MPI. Имени функции предшествует префикс MPI, далее следует одно или несколько слов названия, первое слово в имени функции начинается с заглавного символа, слова разделяются знаком подчеркивания. Названия функций MPI, как правило, поясняют назначение выполняемых функцией действий.

Определение количества процессов в выполняемой параллельной программе осуществляется при помощи функции:

.. code-block:: c

	int MPI_Comm_size(MPI_Comm comm, int *size),
	* comm — коммуникатор, размер которого определяется,
	* size — определяемое количество процессов в коммуникаторе.

Для определения ранга процесса используется функция:

.. code-block:: c

	int MPI_Comm_rank(MPI_Comm comm, int *rank), где
	* comm — коммуникатор, в котором определяется ранг процесса,
	* rank — ранг процесса в коммуникаторе.

Как правило, вызов функций MPI_Comm_size и MPI_Comm_rank выполняется сразу после MPI_Init для получения общего количества процессов и ранга текущего процесса:

.. code-block:: c

	#include "mpi.h"
		int main(int argc, char *argv[]) {
		int ProcNum, ProcRank;
		<программный код без использования функций MPI>
		MPI_Init(&argc, &argv);
		MPI_Comm_size(MPI_COMM_WORLD, &ProcNum);
		MPI_Comm_rank(MPI_COMM_WORLD, &ProcRank);
		<программный код с использованием функций MPI>
		MPI_Finalize();
		<программный код без использования функций MPI>
		return 0;
	}

Следует отметить:

* коммуникатор MPI_COMM_WORLD, создается по умолчанию и представляет все процессы выполняемой параллельной программы;
* ранг, получаемый при помощи функции MPI_Comm_rank, является рангом процесса, выполнившего вызов этой функции, т. е. переменная ProcRank примет различные значения у разных процессов.


Типы данных
===========

MPI допускает возможность запуска процессов параллельной программы на компьютерах различных платформ, обеспечивая при этом
автоматическое преобразование данных при пересылках. В таблице приведено соответствие предопределенных в MPI типов стандартным
типам языка С.

+--------------------+---------------------+
| тип MPI            | тип языка C         |
+--------------------+---------------------+
| MPI_CHAR           | signed char         |
+--------------------+---------------------+
| MPI_SHORT          | signed short int    |
+--------------------+---------------------+
| MPI_INT            | signed int          |
+--------------------+---------------------+
| MPI_LONG           | signed long int     |
+--------------------+---------------------+
| MPI_UNSIGNED_CHAR  | unsigned char       |
+--------------------+---------------------+
| MPI_UNSIGNED_SHORT | unsigned short int  |
+--------------------+---------------------+
| MPI_UNSIGNED       | unsigned int        |
+--------------------+---------------------+
| MPI_UNSIGNED_LONG  | unsigned long int   |
+--------------------+---------------------+
| MPI_FLOAT          | Float               |
+--------------------+---------------------+
| MPI_DOUBLE         | Double              |
+--------------------+---------------------+
| MPI_LONG_DOUBLE    | long double         |
+--------------------+---------------------+
| MPI_BYTE           | --                  |
+--------------------+---------------------+
| MPI_PACKED         | --                  |
+--------------------+---------------------+

В таблице перечислен обязательный минимум поддерживаемых стандартных типов, однако, если в базовой системе представлены и другие типы, то их поддержку будет осуществлять и MPI, например, если в системе есть поддержка комплексных переменных двойной точности DOUBLE COMPLEX, то будет присутствовать тип MPI_DOUBLE_COMPLEX. Типы MPI_BYTE и MPI_PACKED используются для передачи двоичной информации без какого-либо
преобразования. Кроме того, программисту предоставляются средства создания собственных типов на базе стандартных.


Процедуры передачи/приема сообщений между отдельными процессами
===============================================================

Все процедуры передачи сообщений в MPI можно разделить на две группы. В одну группу входят процедуры, которые предназначены для
взаимодействия только двух процессоров программы. Такие операции называются операциями типа точка-точка. Процедуры другой группы
предполагают, что в операцию должны быть вовлечены все процессы некоторого коммуникатора. Такие операции называются коллективными.
Рассмотрим первый тип операций. В таких взаимодействиях участвуют два процесса, один из которых является отправителем сообщения, а второй – получателем. Процесс-отправитель вызывает одну из процедур передачи данных и явно указывает номер получателя в некотором коммуникаторе, а процесс-получатель вызывает одну из процедур приема с указанием того же коммуникатора, причем в некоторых случаях он может не знать точный номер процесса-отправителя в данном коммуникаторе.

Все процедуры данной группы делятся на два класса: процедуры с блокировкой (с синхронизацией) и процедуры без блокировки (асинхронные). Процедуры с блокировкой приостанавливают работу процесса до выполнения некоторого условия, а возврат из асинхронных процедур происходит немедленно после инициализации соответствующей коммуникационной операции. Использование синхронных процедур может привести к тупиковым ситуациям. Использование асинхронных операций к тупиковым ситуациям не приводит, однако требует более аккуратного использования массивов данных.

Обмен сообщениями с блокировкой
===============================

Для передачи сообщения процесс-отправитель должен выполнить функцию:

.. code-block:: c

	int MPI_Send(void *buf, int count, MPI_Datatype type, int dest, int tag, MPI_Comm comm), где
	* buf — адрес буфера памяти, в котором располагаются данные отправляемого сообщения;
	* count — количество элементов данных в сообщении;
	* type — тип элементов данных пересылаемого сообщения;
	* dest — ранг процесса, которому отправляется сообщение;
	* tag — значение-тег, используемое для идентификации сообщения;
	* comm — коммуникатор, в рамках которого выполняется передача данных.


Следует 

* отправляемое сообщение определяется через указание блока памяти (буфера), в котором это сообщение располагается. Используемая для указания буфера триада (buf, count, type) входит в состав параметров практически всех функций передачи данных;
* процессы, между которыми выполняется передача данных, в обязательном порядке должны принадлежать коммуникатору, указываемому в функции MPI_Send;
* параметр tag используется только при необходимости различения передаваемых сообщений, в противном случае в качестве значения параметра может быть использовано произвольное положительное целое число (см. также описание функции MPI_Recv).

Сразу же после завершения функции MPI_Send процесс-отправитель может начать повторно использовать буфер памяти, в
котором располагалось отправляемое сообщение. Также следует понимать, что в момент завершения функции MPI_Send состояние самого
пересылаемого сообщения может быть совершенно различным:
* сообщение может располагаться в процессе-отправителе
* может находиться в состоянии передачи, может храниться в процессе-получателе
* может быть принято процессом-получателем при помощи функции MPI_Recv. 

Тем самым, завершение функции MPI_Send означает лишь, что операция передачи начала выполняться и пересылка сообщения рано
или поздно будет выполнена.
Чтобы расширить возможности передачи сообщений, в MPI введены дополнительные три процедуры. Все параметры у этих процедур такие же, как и у MPI_Send, однако у каждой из них есть своя особенность.

MPI предоставляет следующие модификации процедуры передачи данных с блокировкой MPI_Send:

* MPI_BSend – передача сообщения с буферизацией. Если прием посылаемого сообщения еще не был инициализирован процессом-получателем, то сообщение будет записано в специальный буфер, и произойдет немедленный возврат из процедуры. Выполнение данной процедуры никак не зависит от соответствующего вызова процедуры приема сообщения. Тем не менее, процедура может вернуть код ошибки,  сли места под буфер недостаточно. О выделении массива для буферизации должен заботиться пользователь.
* MPI_SSend – передача сообщений с синхронизацией. Выход из данной процедуры произойдет только тогда, когда прием посылаемого сообщения будет инициализирован процессом-получателем. Таким образом, завершение передачи с синхронизацией говорит не только о возможности повторного использования буфера посылки, но и о гарантированном достижении процессом-получателем точки приема сообщения в программе. Использование передачи сообщений с синхронизацией может замедлить выполнение программы, но позволяет избежать наличия в системе большого количества не принятых буферизованных сообщений.
* MPI_RSend – передача сообщений по готовности. Данной процедурой можно воспользоваться только в том случае, если процесс-получатель уже инициировал прием сообщения. В противном случае вызов процедуры является ошибочным и результат ее выполнения не определен. Гарантировать инициализацию приема сообщения перед вызовом процедуры MPI_RSend можно с помощью операций, осуществляющих явную или неявную синхронизацию процессов (например, MPI_Barrier или MPI_SSend). Во многих реализациях процедура сокращает протокол взаимодействия между отправителем и получателем, уменьшая накладные расходы на организацию передачи данных.

Для приема сообщения процесс-получатель должен выполнить функцию:

.. code-block:: c

	int MPI_Recv(void *buf, int count, MPI_Datatype type, int source, int tag, MPI_Comm comm, MPI_Status *status),
	* buf, count, type — буфер памяти для приема сообщения, назначение каждого отдельного параметра соответствует описанию в MPI_Send;
	* source — ранг процесса, от которого должен быть выполнен прием сообщения;
	* tag — тег сообщения, которое должно быть принято для процесса;
	* comm — коммуникатор, в рамках которого выполняется передача данных;
	* status – указатель на структуру данных с информацией о результате выполнения операции приема данных.

Следует отметить:

* буфер памяти должен быть достаточным для приема сообщения. При нехватке памяти часть сообщения будет потеряна и в коде завершения функции будет зафиксирована ошибка переполнения; с другой стороны, принимаемое сообщение может быть и короче, чем размер приемного буфера, в таком случае изменятся только участки буфера, затронутые принятым сообщением;
* типы элементов передаваемого и принимаемого сообщения должны совпадать;
* при необходимости приема сообщения от любого процесса-отправителя для параметра source может быть указано значение 

MPI_ANY_SOURCE (в отличие от функции передачи MPI_Send, которая отсылает сообщение строго определенному адресату);

* при необходимости приема сообщения с любым тегом для параметра tag может быть указано значение MPI_ANY_TAG (при использовании функции 

MPI_Send должно быть указано конкретное значение тега);

* в отличие от параметров "процесс-получатель" и "тег", параметр "коммуникатор" не имеет значения, означающего "любой коммуникатор";
* параметр status позволяет определить ряд характеристик принятого сообщения: status.MPI_SOURCE — ранг процесса-отправителя принятого

сообщения; status.MPI_TAG — тег принятого сообщения.

Приведенные значения MPI_ANY_SOURCE и MPI_ANY_TAG иногда называют джокерами.

Значение переменной status позволяет определить количество элементов данных в принятом сообщении при помощи функции:

.. code-block:: c

	int MPI_Get_count(MPI_Status *status, MPI_Datatype type,int *count), где
	* status — статус операции MPI_Recv;
	* type — тип принятых данных;
	* count — количество элементов данных в сообщении.


Вызов функции MPI_Recv не обязан быть согласованным со временем вызова соответствующей функции передачи сообщения MPI_Send – прием сообщения может быть инициирован до момента, в момент или после момента начала отправки сообщения. По завершении функции MPI_Recv в заданном буфере памяти будет располагаться принятое сообщение. Так как функция MPI_Recv является блокирующей для процесса-получателя, его выполнение приостанавливается до завершения работы функции. Таким образом, если по каким-то причинам ожидаемое для приема сообщение будет
отсутствовать, выполнение параллельной программы будет блокировано.

Обмен сообщениями без блокировки
================================

В отличие от функций с блокировкой, возврат из функций данной группы происходит сразу без какой-либо блокировки процессов. На фоне дальнейшего выполнения программы одновременно происходит и обработка асинхронно запущенной операции. Данная возможность полезна для создания эффективных программ. В самом деле, программист знает, что в некоторый момент ему потребуется массив, который вычисляет другой процесс. Он заранее выставляет в программе асинхронный запрос на получение данного массива, а до того момента, когда массив реально потребуется, он может выполнять любую другую полезную работу. Опять же, во многих случаях совершенно не обязательно дожидаться окончания посылки сообщения для выполнения последующих вычислений.

.. code-block:: c

	int MPI_Isend(void* buf, int count, MPI_Datatype type,int dest, int tag,MPI_Comm comm, MPI_Request *request)
	* buf — адрес буфера памяти, в котором располагаются данные отправляемого сообщения;
	* count — количество элементов данных в сообщении;
	* type — тип элементов данных пересылаемого сообщения;
	* dest — ранг процесса, которому отправляется сообщение;
	* tag — значение-тег, используемое для идентификации сообщения;
	* comm — коммуникатор, в рамках которого выполняется передача данных;
	* request – имя (заголовка) запроса

Неблокированая передача данных инициализирует посылающее действие, но не заканчивает его. Функция возвратит управление прежде, чем сообщение скопировано вне посылающегося буфера. Неблокированная посылающая функция указывает, что система может начинать копировать данные вне посылающегося буфера. Посылающий процесс не должен иметь доступа к посылаемому буферу после того, как неблокированное посылающее действие инициировано, до тех пор, пока функция завершения не возвратит управление.

.. code-block:: c

	int MPI_Irecv(void* buf, int count, MPI_Datatype type, int source, int tag, MPI_Comm comm, MPI_Request *request)
	* buf — адрес буфера памяти, в котором располагаются данные получаемого сообщения;
	* count — количество элементов данных в сообщении;
	* type — тип элементов данных;
	* source — ранг процесса, от которого должен быть выполнен прием сообщения;
	* tag — тег сообщения, которое должно быть принято для процесса;
	* comm — коммуникатор, в рамках которого выполняется передача данных;
	* request – имя (заголовка) запроса.

Неблокированный прием данных инициализирует получающее действие, но не заканчивает его. Функция возвратит управление прежде, чем сообщение записано в буфер приема данных. Неблокированная получающая функция указывает, что система может начинать писать данные в буфер приема данных. Приемник не должен иметь доступа к буферу приема после того, как неблокированное получающее действие инициировано, до тех пор, пока функция завершения не возвратит управление.
Эти обе функции размещают данные в системном буфере и возвращают заголовок этого запроса в request. Request используется, чтобы опросить состояние связи. Чтобы закончить неблокированные посылку и получение данных, используются завершающие функции MPI_Wait и MPI_Test. Завершение посылающего процесса указывает, что он теперь свободен к доступу посылающегося буфера. Завершение получающего процесса указывает,
что буфер приема данных содержит сообщение, приемник свободен к его доступу.

.. code-block:: c

	int MPI_Wait(MPI_Request *request, MPI_Status *status)
	* request – имя запроса;
	* status – статус объекта;

Запрос к MPI_Wait возвращает управление после того, как операция, идентифицированная request, выполнилась. Это блокированная функция. Если объект системы, указанный request, был первоначально создан неблокированными посылающей или получающей функциями, то этот объект освобождается функцией MPI_Wait, и request устанавливается в MPI_REQUEST_NULL. Статус объекта содержит информацию относительно выполненной операции.

.. code-block:: c

	int MPI_Test(MPI_Request *request, int *flag, MPI_Status *status)
	* request – имя запроса;
	* flag – true, если операция выполнилась, иначе false;
	* status – статус объекта

Запрос к MPI_TEST возвращает flag = true, если операция, идентифицированная request, выполнилась. В этом случае статус состояния содержит информацию относительно законченной операции. Если объект системы, указанный request, был первоначально создан неблокированными посылающей или получающей функциями, то этот объект освобождается функцией MPI_TEST, и request устанавливается в MPI_REQUEST_NULL.
Запрос возвращает flag = false, если операция не выполнилась. В этом случае значение статуса состояния не определено. Это неблокированная функция.

Обмен сообщениями при помощи одной функции
==========================================

В MPI есть группа процедур, совмещающих функции приема и передачи. Они достаточно часто применяются при программировании "каскадных" или "линейных" схем, когда необходимо осуществлять обмен однотипными данными между процессорами. Примером является функция:

.. code-block:: c

	int MPI_Sendrecv (void* sendbuffer, int sendcount, MPI_Datatype senddatatype, int dest, int sendtag, void* recvbuffer, int recvcount, MPI_Datatype recvdatatype, int src, int recvtag MPI_Comm comm, MPI_Status* status), где
	* sendbuffer – адрес массива передаваемых данных;
	* sendcount – количество элементов в массиве;
	* senddatatype – тип передаваемых элементов;
	* dest – ранг адресата;
	* sendtag – тег передаваемого сообщения;
	* recvbuffer – адрес буфера для приема;
	* recvcount – количество элементов в буфере приема;
	* recvdatatype – тип элементов в буфере приема;
	* src – ранг источника;
	* recvtag – тег принимаемого сообщения;
	* comm – коммуникатор;
	* status – структура с дополнительной информацией.

Функция копирует данные из массива sendbuffer процесса с рангом src в буфер recvbuffer процесса с рангом dest.
Другая функция:

.. code-block:: c

	int MPI_Sendrecv_replace (void* buffer, int count, MPI_Datatype datatype, int dest, int sendtag, int src, int recvtag MPI_Comm comm, MPI_Status* status).

Использует только один буфер, также передавая данные с процесса src на процесс dest


Передача данных от одного процесса всем. Широковещательная рассылка данных (broadcast)
======================================================================================
 
 При программировании параллельных задач часто возникает необходимость разослать какую-то порцию данных всем процессам сразу. Очевидно, что для решения этой задачи можно воспользоваться рассмотренными ранее операциями двупроцессного обмена.

.. code-block:: c

	MPI_Comm_size(MPI_COMM_WORLD, &ProcNum);
	for (int i = 1; i < ProcNum; i++) {
		MPI_Send(&x, n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);
	}

 Однако, такое решение неэффективно вследствие значительных затрат на синхронизацию процессов. Поэтому в MPI появилась специальная операция - операция широковещательной рассылки 

.. code-block:: c

	int MPI_Bcast(void *buf, int count, MPI_Datatype type, int root, MPI_Comm comm), где
	* buf, count, type — буфер памяти с отправляемым сообщением (для процесса с рангом 0) и для приема сообщений (для всех остальных процессов);
	* root — ранг процесса, выполняющего рассылку данных;
	* comm — коммуникатор, в рамках которого выполняется передача данных.

Функция MPI_Bcast осуществляет рассылку данных из буфера buf, содержащего count элементов типа type, с процесса, имеющего номер root,
всем процессам, входящим в коммуникатор comm.

Следует отметить:

* функция MPI_Bcast определяет коллективную операцию, и, тем самым, при выполнении необходимых рассылок данных вызов функции MPI_Bcast должен быть осуществлен всеми процессами указываемого коммуникатора;
* указываемый в функции MPI_Bcast буфер памяти имеет различное назначение у разных процессов: для процесса с рангом root, которым осуществляется рассылка данных, в этом буфере должно находиться рассылаемое сообщение, а для всех остальных процессов указываемый буфер предназначен для приема передаваемых данных;
* все коллективные операции "несовместимы" с парными операциями — так, например, принять широковещательное сообщение, отосланное с помощью MPI_Bcast, функцией MPI_Recv нельзя, для этого можно задействовать только MPI_Bcast.

Передача данных от всех процессов одному. Операции редукции
===========================================================

MPI предоставляет обратную по отношению к широковещательной рассылке операцию - операцию сбора данных или редукцию. Операция редукции позволяет, собрав на одном из узлов данные, посланные остальными узлами, выполнить над ними какую-либо из групповых операций - типа сложения, поиска максимума, минимума, среднего значения и т.д.

.. code-block:: c

	int MPI_Reduce(void *sendbuf, void *recvbuf, int count, MPI_Datatype type, MPI_Op op, int root, MPI_Comm comm), где
	* sendbuf — буфер памяти с отправляемым сообщением;
	* recvbuf — буфер памяти для результирующего сообщения (только для процесса с рангом root);
	* count — количество элементов в сообщениях;
	* type — тип элементов сообщений;
	* op — операция, которая должна быть выполнена над данными;
	* root — ранг процесса, на котором должен быть получен результат;
	* comm — коммуникатор, в рамках которого выполняется операция.

В качестве операций редукции данных могут быть использованы предопределенные в MPI операции:

+----------+-------------------------------------+
| Операции | Описание                            |
+----------+-------------------------------------+
| MPI_MAX  | Определение максимального значения  |
+----------+-------------------------------------+
| MPI_MIN  | Определение минимального значения   |
+----------+-------------------------------------+
| MPI_SUM  | Определение суммы значений          |
+----------+-------------------------------------+
| MPI_PROD | Определение произведения значений   |
+----------+-------------------------------------+


 Элементы получаемого сообщения на процессе root представляют собой результаты обработки соответствующих элементов передаваемых процессами сообщений.

* функция MPI_Reduce определяет коллективную операцию, и, тем самым, вызов функции должен быть выполнен всеми процессами указываемого коммуникатора. При этом все вызовы функции должны содержать одинаковые значения параметров count, type, op, root, comm;
* передача сообщений должна быть выполнена всеми процессами, результат операции будет получен только процессом с рангом root;
* выполнение операции редукции осуществляется над отдельными элементами передаваемых сообщений. Так, например, если сообщения содержат по два элемента данных и выполняется операция суммирования MPI_SUM, то результат также будет состоять из двух значений, первое из которых будет содержать сумму первых элементов всех отправленных сообщений, а второе значение будет равно сумме вторых элементов сообщений соответственно.

Функции распределения и сбора данных
====================================

При программировании часто возникает задача распределения массива данных по процессам некоторыми регулярными "кусками". Например, распределение матрицы, нарезанной вертикальными лентами. Возникает и обратная задача – сбор на некотором выделенном процессе
некоторого набора данных, распределенного по всем процессам.

Распределение и сбор данных осуществляется с помощью вызовов процедур MPI_Scatter и MPI_Gather:

.. code-block:: c

	int MPI_Scatter(void* sendbuf, int sentcount, MPI_Datatype senddatatype, void* recbuf, int reccount, MPI_Datatype recdatatype,int root,MPI_Comm comm), где:
	* sendbuf – адрес буфера для передачи данных;
	* sentcount – количество элементов, передаваемых на каждый процесс (общее количество элементов в буфере равно произведению sentcount на количество процессов в коммуникаторе);
	* senddatatype – тип передаваемых данных;
	* recbuf – буфер для приема данных;
	* reccount – размер буфера recbuf;
	* recdatatype – тип данных для приемки;
	* root – ранг процесса, с которого рассылаются данные;
	* comm – коммуникатор.

При вызове этой процедуры произойдет следующее. Процесс с рангом root произведет передачу данных всем другим процессам в коммуникаторе. Каждому процессу будет отправлено sendcount элементов. Процесс с рангом 0 получит порцию из sendbuf, начиная с 0-го и заканчивая sendcount-1 элементом. Процесс с рангом 1 получит порцию, начиная с sendcount, заканчивая 2* sendcount-1 и т.д.
Подпрограмма MPI_Gather собирает данные от остальных процессов.

.. code-block:: c

	int MPI_Gather(void* sendbuf, int sentcount, MPI_Datatype senddatatype, void* recbuf, int reccount, MPI_Datatype recdatatype,int root,MPI_Comm comm), где:
	* sendbuf – адрес буфера для передачи данных;
	* sentcount – количество элементов, передаваемое на главный процесс; 
	* senddatatype – тип передаваемых данных;
	* recbuf – буфер для приема данных;
	* reccount – размер буфера recbuf;
	* recdatatype – тип данных для приемки;
	* root – ранг процесса, на котором собираются данные;
	* comm – коммуникатор.

Посредством MPI_Gather каждый процесс в коммуникаторе передает данные из буфера sendbuf на процесс с рангом root. Этот "ведущий" процесс осуществляет склейку поступающих данных в буфере recbuf. Склейка данных осуществляется линейно, положение пришедшего фрагмента данных определяется рангом процесса, его приславшего. В целом процедура MPI_Gather обратна по своему действию процедуре MPI_Scatter.
Следует заметить, что при использовании MPI_Gather сборка осуществляется только на одном процессе. Во всех остальных процессах
заполнение буфера recbuf не определено. Для некоторых задач необходимо, чтобы данные, рассчитанные на каждом из процессов, были
собраны в единый объект опять же на каждом процессе. В таком случае, вместо функции MPI_Gather следует использовать функцию
MPI_Allgather. 
При использовании функции MPI_Allgather на всех процессах в буфере recbuf будут собраны одинаковые данные - "большой" объект, полученный как объединение фрагментов, переданных с каждого из процессов.

Другая полезная процедура MPI_Alltoall пересылает данные по принципу "все - всем"

Кроме перечисленных, в MPI существует еще несколько функций, осуществляющих различные коллективные операции. При работе с ними следует помнить следующие основные моменты: 

* все коллективные операции выполняются в рамках коммуникатора. Если необходимо выполнить коллективную операцию над подмножеством процессов, следует создать для этой цели свой коммуникатор.
* коллективные операции должны вызываться во всех процессах, которые в них участвуют.
* разумное использование коллективных операций - хорошее средство повышения производительности
  
Пример
======

Скомпилируйте и запустите программу, производящую суммирование элементов массива:

.. code-block:: c

	#include <math.h>
	#include <stdio.h>
	#include <stdlib.h>
	#include "mpi.h"

	int main( int argc, char* argv[] ) {
		const int N = 100;
		double x[N], TotalSum, ProcSum = 0.0;
		int ProcRank, ProcNum, k, i1, i2;
		MPI_Status Status;
	
		// Инициализация
		MPI_Init( &argc, &argv );
		MPI_Comm_size( MPI_COMM_WORLD, &ProcNum );
		MPI_Comm_rank( MPI_COMM_WORLD, &ProcRank );
		
		// Подготовка данных
		if ( ProcRank == 0 ) {
			for( i1 = 0; i1 < N; ++i1 ) {
				x[i1] = i1;
			}
		}
		
		// Рассылка данных на все процессы
		MPI_Bcast( x, N, MPI_DOUBLE, 0, MPI_COMM_WORLD );

		// Вычисление частичной суммы на каждом из процессов
		// на каждом процессе суммируются элементы вектора x от i1 до i2
		k = N / ProcNum;
		i1 = k * ProcRank;
		i2 = k * ( ProcRank + 1 );
		if ( ProcRank == ProcNum-1 ) i2 = N;
		for ( int i = i1; i < i2; i++ ) {
			ProcSum = ProcSum + x[i];
		}
		
		MPI_Reduce( &ProcSum, &TotalSum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD );
	
		// Вывод результата
		if ( ProcRank == 0 ) {
			printf("\nTotal Sum = %10.2f",TotalSum);
		}
		MPI_Finalize();
		return 0;
	}



Сортировка
==========


Рассмотрим простейший алгоритм - последовательный алгоритм пузырьковой сортировки. Принцип его работы простой - сравниваются и обмениваются соседние элементы в последовательности, которую нужно отсортировать. После N-1 итераций исходный массив будет отсортирован.

Алгоритм пузырьковой сортировки в прямом виде достаточно сложен для распараллеливания – сравнение пар значений упорядочиваемого набора данных происходит строго последовательно. В связи с этим для параллельного применения используется не сам этот алгоритм, а его модификация, известная в литературе как метод чет-нечетной перестановки. 
Суть модификации состоит в том, что в алгоритм сортировки вводятся два разных правила выполнения итераций метода: в зависимости от четности или нечетности номера итерации сортировки для обработки выбираются элементы с четными или нечетными индексами соответственно, сравнение выделяемых значений всегда осуществляется с их правыми соседними элементами:

.. code-block:: c

	void OddEvenSort (int a[], int n) {
		for( int i = 0; i < n; i++ ) {
			if (i & 1) {
			for( int j = 2; j < n; j += 2 ) {
				if ( a[j] < a[j-1] )
					swap( a[j-1], a[j] );
				}
			} else {
			for( int j = 1; j < n; j += 2 ) {
				if ( a[j] < a[j-1] ) {
					swap (a[j-1], a[j]) ;
				}
			}
		}
	}

Задание: Реализуйте параллельную версию алгоритма с использованием MPI.

Домашнее задание
----------------

Аналогично примеру с суммированием массива реализовать интегрирование заданной функции на отрезке методом трапеций с заданным шагом.
Измерить с помощью MPI_Wtime() время исполнения на 4, 8, 16 и 28 узлах для 2 вариантов реализации:

#. В случае, когда процессам рассылаются интервалы, за которые они отвечают.
#. Когда процессы сами вычисляют границы своих интервалов

Есть ли разница во времени исполнения? Объяснить её. 
